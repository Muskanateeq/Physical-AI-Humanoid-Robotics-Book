"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[528],{50:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"module-4-vla/voice-to-action-whisper","title":"Chapter 12: Voice-to-Action with Whisper","description":"Now that our hardware is configured, it\'s time to build the first part of our VLA pipeline","source":"@site/docs/module-4-vla/voice-to-action-whisper.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/voice-to-action-whisper","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/voice-to-action-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/Muskanateeq/Physical-AI-Humanoid-Robotics-Book/tree/main/docs/module-4-vla/voice-to-action-whisper.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"voice-to-action-whisper","title":"Chapter 12: Voice-to-Action with Whisper","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 11: Introduction and Hardware Setup","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/introduction-and-setup"},"next":{"title":"Chapter 13: LLM Cognitive Planning","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/llm-cognitive-planning"}}');var t=i(4848),o=i(8453),s=i(7293);const a={id:"voice-to-action-whisper",title:"Chapter 12: Voice-to-Action with Whisper",sidebar_position:2},c="2. Voice-to-Action Pipeline with Whisper",l={},d=[{value:"Step 1: Install Whisper",id:"step-1-install-whisper",level:2},{value:"Step 2: Create the ROS 2 Node (<code>voice_transcriber_node.py</code>)",id:"step-2-create-the-ros-2-node-voice_transcriber_nodepy",level:2},{value:"How It Works",id:"how-it-works",level:3},{value:"Exercise: Test the Transcription Node",id:"exercise-test-the-transcription-node",level:3}];function h(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"2-voice-to-action-pipeline-with-whisper",children:"2. Voice-to-Action Pipeline with Whisper"})}),"\n",(0,t.jsx)(n.p,{children:"Now that our hardware is configured, it's time to build the first part of our VLA pipeline: the robot's hearing. In this section, we will create a ROS 2 node that:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Listens for audio from the ReSpeaker microphone."}),"\n",(0,t.jsxs)(n.li,{children:["Uses the powerful ",(0,t.jsx)(n.strong,{children:"OpenAI Whisper"})," model to transcribe speech into text in real-time."]}),"\n",(0,t.jsx)(n.li,{children:"Publishes the transcribed text onto a ROS 2 topic for other parts of the robot's brain to use."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"step-1-install-whisper",children:"Step 1: Install Whisper"}),"\n",(0,t.jsxs)(n.p,{children:["First, we need to install the ",(0,t.jsx)(n.code,{children:"openai-whisper"})," library and its required dependency ",(0,t.jsx)(n.code,{children:"ffmpeg"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Install the python package\r\npip install openai-whisper\r\n\r\n# Install the system dependency\r\nsudo apt-get update && sudo apt-get install -y ffmpeg\n"})}),"\n",(0,t.jsxs)(n.h2,{id:"step-2-create-the-ros-2-node-voice_transcriber_nodepy",children:["Step 2: Create the ROS 2 Node (",(0,t.jsx)(n.code,{children:"voice_transcriber_node.py"}),")"]}),"\n",(0,t.jsx)(n.p,{children:"Create a new Python file in your ROS 2 package's source directory. This node will contain all the logic for our voice transcription service."}),"\n",(0,t.jsxs)(s.A,{type:"danger",title:"CRITICAL: API Key Security",children:[(0,t.jsxs)(n.p,{children:["If you were using the Whisper API directly (instead of a local model), you would need an API key. As mandated by the course Constitution, keys ",(0,t.jsx)(n.strong,{children:"MUST NEVER"})," be hardcoded. They should always be loaded from environment variables. For a local model like this, no key is needed, but the principle is critical for the next section."]}),(0,t.jsx)(n.p,{children:"Example of safe key handling:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import os\r\n# This is how you would safely load a key if needed\r\n# api_key = os.getenv("OPENAI_API_KEY")\r\n# if not api_key:\r\n#     raise ValueError("OPENAI_API_KEY environment variable not set.")\n'})})]}),"\n",(0,t.jsx)(n.p,{children:"Here is the complete code for the node. We will break down how it works below."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# voice_transcriber_node.py\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport numpy as np\r\nimport sounddevice as sd\r\nimport whisper\r\n\r\nclass VoiceTranscriberNode(Node):\r\n    def __init__(self):\r\n        super().__init__('voice_transcriber_node')\r\n        self.get_logger().info('Voice Transcriber Node started.')\r\n\r\n        # ROS 2 Publisher for the transcribed command\r\n        self.publisher_ = self.create_publisher(String, '/voice_command', 10)\r\n\r\n        # --- Whisper Model Configuration ---\r\n        self.model_name = \"base.en\"  # \"base.en\" is a good balance for the Jetson Orin\r\n        self.get_logger().info(f\"Loading Whisper model: {self.model_name}...\")\r\n        self.model = whisper.load_model(self.model_name)\r\n        self.get_logger().info(\"Whisper model loaded successfully.\")\r\n\r\n        # --- Audio Stream Configuration ---\r\n        self.sample_rate = 16000  # 16kHz\r\n        self.channels = 1\r\n        self.block_size = 4096 # Buffer size for each audio chunk\r\n        \r\n        # Start listening\r\n        self.stream = sd.InputStream(\r\n            callback=self.audio_callback,\r\n            samplerate=self.sample_rate,\r\n            channels=self.channels,\r\n            blocksize=self.block_size,\r\n            dtype='int16' # Use int16, then convert to float32 for Whisper\r\n        )\r\n        self.stream.start()\r\n        self.get_logger().info(\"Listening for voice commands...\")\r\n\r\n    def audio_callback(self, indata, frames, time, status):\r\n        \"\"\"\r\n        This function is called by the sounddevice stream for each new audio chunk.\r\n        \"\"\"\r\n        if status:\r\n            self.get_logger().warn(f'Audio callback status: {status}')\r\n\r\n        # --- Simple Voice Activity Detection (VAD) ---\r\n        # We check the energy of the audio chunk to see if it's loud enough to be speech.\r\n        volume_norm = np.linalg.norm(indata) * 10\r\n        if volume_norm > 100: # This threshold may need tuning\r\n            self.get_logger().info(f'Speech detected! (Volume: {volume_norm:.2f}) Transcribing...')\r\n\r\n            # Convert audio data from int16 to float32 as required by Whisper\r\n            audio_float32 = indata.flatten().astype(np.float32) / 32768.0\r\n            \r\n            # Transcribe the audio chunk\r\n            result = self.model.transcribe(audio_float32)\r\n            transcribed_text = result['text'].strip()\r\n\r\n            if transcribed_text:\r\n                self.get_logger().info(f\"Whisper transcribed: '{transcribed_text}'\")\r\n                \r\n                # Publish the transcribed text to the ROS 2 topic\r\n                msg = String()\r\n                msg.data = transcribed_text\r\n                self.publisher_.publish(msg)\r\n        else:\r\n            # This is useful for debugging your microphone and VAD threshold\r\n            # self.get_logger().info(f'Silence detected. (Volume: {volume_norm:.2f})')\r\n            pass\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    voice_transcriber_node = VoiceTranscriberNode()\r\n    try:\r\n        rclpy.spin(voice_transcriber_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        voice_transcriber_node.stream.stop()\r\n        voice_transcriber_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\n"})}),"\n",(0,t.jsx)(n.h3,{id:"how-it-works",children:"How It Works"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Initialization"}),": The ",(0,t.jsx)(n.code,{children:"VoiceTranscriberNode"})," class is initialized. It creates a ROS 2 publisher for the ",(0,t.jsx)(n.code,{children:"/voice_command"})," topic and loads the specified Whisper model (",(0,t.jsx)(n.code,{children:"base.en"}),") into memory."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Stream"}),": It opens a ",(0,t.jsx)(n.code,{children:"sounddevice.InputStream"}),". This is a live feed from your default microphone (which we set to the ReSpeaker array). For every chunk of audio it records, it calls the ",(0,t.jsx)(n.code,{children:"audio_callback"})," function."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice Activity Detection (VAD)"}),": Inside ",(0,t.jsx)(n.code,{children:"audio_callback"}),', we calculate the "energy" or volume of the audio chunk. If it\'s above a certain threshold, we assume someone is speaking and proceed to transcribe. This prevents the model from wasting resources trying to transcribe silence.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transcription"}),": The audio data is converted to the format Whisper expects (a float32 NumPy array) and passed to ",(0,t.jsx)(n.code,{children:"model.transcribe()"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Publishing"}),": The resulting text is cleaned up and published as a ",(0,t.jsx)(n.code,{children:"std_msgs/String"})," on the ",(0,t.jsx)(n.code,{children:"/voice_command"})," topic."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-test-the-transcription-node",children:"Exercise: Test the Transcription Node"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Save the code above as ",(0,t.jsx)(n.code,{children:"voice_transcriber_node.py"})," inside your ROS 2 package."]}),"\n",(0,t.jsxs)(n.li,{children:["Build your package (",(0,t.jsx)(n.code,{children:"colcon build"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:["Source your workspace (",(0,t.jsx)(n.code,{children:"source install/setup.bash"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:["In one terminal, run your new node:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 run your_package_name voice_transcriber_node\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["In a second terminal, listen to the ",(0,t.jsx)(n.code,{children:"/voice_command"})," topic:","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /voice_command\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:['Speak a command into the ReSpeaker microphone, such as "Hello robot, what time is it?". You should see the node terminal log "Speech detected!" and then see the transcribed text appear in the ',(0,t.jsx)(n.code,{children:"ros2 topic echo"})," terminal."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["If you see your words appear, congratulations! Your robot can now hear. In the next section, we will teach it how to ",(0,t.jsx)(n.em,{children:"understand"}),"."]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},7293:(e,n,i)=>{i.d(n,{A:()=>V});var r=i(6540),t=i(4848);function o(e){const{mdxAdmonitionTitle:n,rest:i}=function(e){const n=r.Children.toArray(e),i=n.find(e=>r.isValidElement(e)&&"mdxAdmonitionTitle"===e.type),o=n.filter(e=>e!==i),s=i?.props.children;return{mdxAdmonitionTitle:s,rest:o.length>0?(0,t.jsx)(t.Fragment,{children:o}):null}}(e.children),o=e.title??n;return{...e,...o&&{title:o},children:i}}var s=i(4164),a=i(1312),c=i(7559);const l="admonition_xJq3",d="admonitionHeading_Gvgb",h="admonitionIcon_Rf37",u="admonitionContent_BuS1";function p({type:e,className:n,children:i}){return(0,t.jsx)("div",{className:(0,s.A)(c.G.common.admonition,c.G.common.admonitionType(e),l,n),children:i})}function m({icon:e,title:n}){return(0,t.jsxs)("div",{className:d,children:[(0,t.jsx)("span",{className:h,children:e}),n]})}function f({children:e}){return e?(0,t.jsx)("div",{className:u,children:e}):null}function x(e){const{type:n,icon:i,title:r,children:o,className:s}=e;return(0,t.jsxs)(p,{type:n,className:s,children:[r||i?(0,t.jsx)(m,{title:r,icon:i}):null,(0,t.jsx)(f,{children:o})]})}function g(e){return(0,t.jsx)("svg",{viewBox:"0 0 14 16",...e,children:(0,t.jsx)("path",{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"})})}const j={icon:(0,t.jsx)(g,{}),title:(0,t.jsx)(a.A,{id:"theme.admonition.note",description:"The default label used for the Note admonition (:::note)",children:"note"})};function v(e){return(0,t.jsx)(x,{...j,...e,className:(0,s.A)("alert alert--secondary",e.className),children:e.children})}function b(e){return(0,t.jsx)("svg",{viewBox:"0 0 12 16",...e,children:(0,t.jsx)("path",{fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"})})}const _={icon:(0,t.jsx)(b,{}),title:(0,t.jsx)(a.A,{id:"theme.admonition.tip",description:"The default label used for the Tip admonition (:::tip)",children:"tip"})};function y(e){return(0,t.jsx)(x,{..._,...e,className:(0,s.A)("alert alert--success",e.className),children:e.children})}function w(e){return(0,t.jsx)("svg",{viewBox:"0 0 14 16",...e,children:(0,t.jsx)("path",{fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"})})}const k={icon:(0,t.jsx)(w,{}),title:(0,t.jsx)(a.A,{id:"theme.admonition.info",description:"The default label used for the Info admonition (:::info)",children:"info"})};function A(e){return(0,t.jsx)(x,{...k,...e,className:(0,s.A)("alert alert--info",e.className),children:e.children})}function N(e){return(0,t.jsx)("svg",{viewBox:"0 0 16 16",...e,children:(0,t.jsx)("path",{fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"})})}const I={icon:(0,t.jsx)(N,{}),title:(0,t.jsx)(a.A,{id:"theme.admonition.warning",description:"The default label used for the Warning admonition (:::warning)",children:"warning"})};function S(e){return(0,t.jsx)("svg",{viewBox:"0 0 12 16",...e,children:(0,t.jsx)("path",{fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"})})}const T={icon:(0,t.jsx)(S,{}),title:(0,t.jsx)(a.A,{id:"theme.admonition.danger",description:"The default label used for the Danger admonition (:::danger)",children:"danger"})};const C={icon:(0,t.jsx)(N,{}),title:(0,t.jsx)(a.A,{id:"theme.admonition.caution",description:"The default label used for the Caution admonition (:::caution)",children:"caution"})};const R={...{note:v,tip:y,info:A,warning:function(e){return(0,t.jsx)(x,{...I,...e,className:(0,s.A)("alert alert--warning",e.className),children:e.children})},danger:function(e){return(0,t.jsx)(x,{...T,...e,className:(0,s.A)("alert alert--danger",e.className),children:e.children})}},...{secondary:e=>(0,t.jsx)(v,{title:"secondary",...e}),important:e=>(0,t.jsx)(A,{title:"important",...e}),success:e=>(0,t.jsx)(y,{title:"success",...e}),caution:function(e){return(0,t.jsx)(x,{...C,...e,className:(0,s.A)("alert alert--warning",e.className),children:e.children})}}};function V(e){const n=o(e),i=(r=n.type,R[r]||(console.warn(`No admonition component found for admonition type "${r}". Using Info as fallback.`),R.info));var r;return(0,t.jsx)(i,{...n})}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var r=i(6540);const t={},o=r.createContext(t);function s(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);