"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[29],{7293:(n,e,t)=>{t.d(e,{A:()=>C});var i=t(6540),r=t(4848);function o(n){const{mdxAdmonitionTitle:e,rest:t}=function(n){const e=i.Children.toArray(n),t=e.find(n=>i.isValidElement(n)&&"mdxAdmonitionTitle"===n.type),o=e.filter(n=>n!==t),a=t?.props.children;return{mdxAdmonitionTitle:a,rest:o.length>0?(0,r.jsx)(r.Fragment,{children:o}):null}}(n.children),o=n.title??e;return{...n,...o&&{title:o},children:t}}var a=t(4164),s=t(1312),l=t(7559);const c="admonition_xJq3",d="admonitionHeading_Gvgb",h="admonitionIcon_Rf37",p="admonitionContent_BuS1";function u({type:n,className:e,children:t}){return(0,r.jsx)("div",{className:(0,a.A)(l.G.common.admonition,l.G.common.admonitionType(n),c,e),children:t})}function m({icon:n,title:e}){return(0,r.jsxs)("div",{className:d,children:[(0,r.jsx)("span",{className:h,children:n}),e]})}function f({children:n}){return n?(0,r.jsx)("div",{className:p,children:n}):null}function g(n){const{type:e,icon:t,title:i,children:o,className:a}=n;return(0,r.jsxs)(u,{type:e,className:a,children:[i||t?(0,r.jsx)(m,{title:i,icon:t}):null,(0,r.jsx)(f,{children:o})]})}function x(n){return(0,r.jsx)("svg",{viewBox:"0 0 14 16",...n,children:(0,r.jsx)("path",{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"})})}const j={icon:(0,r.jsx)(x,{}),title:(0,r.jsx)(s.A,{id:"theme.admonition.note",description:"The default label used for the Note admonition (:::note)",children:"note"})};function v(n){return(0,r.jsx)(g,{...j,...n,className:(0,a.A)("alert alert--secondary",n.className),children:n.children})}function _(n){return(0,r.jsx)("svg",{viewBox:"0 0 12 16",...n,children:(0,r.jsx)("path",{fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"})})}const y={icon:(0,r.jsx)(_,{}),title:(0,r.jsx)(s.A,{id:"theme.admonition.tip",description:"The default label used for the Tip admonition (:::tip)",children:"tip"})};function b(n){return(0,r.jsx)(g,{...y,...n,className:(0,a.A)("alert alert--success",n.className),children:n.children})}function A(n){return(0,r.jsx)("svg",{viewBox:"0 0 14 16",...n,children:(0,r.jsx)("path",{fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"})})}const w={icon:(0,r.jsx)(A,{}),title:(0,r.jsx)(s.A,{id:"theme.admonition.info",description:"The default label used for the Info admonition (:::info)",children:"info"})};function L(n){return(0,r.jsx)(g,{...w,...n,className:(0,a.A)("alert alert--info",n.className),children:n.children})}function N(n){return(0,r.jsx)("svg",{viewBox:"0 0 16 16",...n,children:(0,r.jsx)("path",{fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"})})}const I={icon:(0,r.jsx)(N,{}),title:(0,r.jsx)(s.A,{id:"theme.admonition.warning",description:"The default label used for the Warning admonition (:::warning)",children:"warning"})};function k(n){return(0,r.jsx)("svg",{viewBox:"0 0 12 16",...n,children:(0,r.jsx)("path",{fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"})})}const P={icon:(0,r.jsx)(k,{}),title:(0,r.jsx)(s.A,{id:"theme.admonition.danger",description:"The default label used for the Danger admonition (:::danger)",children:"danger"})};const M={icon:(0,r.jsx)(N,{}),title:(0,r.jsx)(s.A,{id:"theme.admonition.caution",description:"The default label used for the Caution admonition (:::caution)",children:"caution"})};const T={...{note:v,tip:b,info:L,warning:function(n){return(0,r.jsx)(g,{...I,...n,className:(0,a.A)("alert alert--warning",n.className),children:n.children})},danger:function(n){return(0,r.jsx)(g,{...P,...n,className:(0,a.A)("alert alert--danger",n.className),children:n.children})}},...{secondary:n=>(0,r.jsx)(v,{title:"secondary",...n}),important:n=>(0,r.jsx)(L,{title:"important",...n}),success:n=>(0,r.jsx)(b,{title:"success",...n}),caution:function(n){return(0,r.jsx)(g,{...M,...n,className:(0,a.A)("alert alert--warning",n.className),children:n.children})}}};function C(n){const e=o(n),t=(i=e.type,T[i]||(console.warn(`No admonition component found for admonition type "${i}". Using Info as fallback.`),T.info));var i;return(0,r.jsx)(t,{...e})}},7351:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4-vla/llm-cognitive-planning","title":"Chapter 13: LLM Cognitive Planning","description":"Your robot can now hear commands. But how does it understand \\"clean the room\\" and know what actions to take? This is where Cognitive Planning using a Large Language Model (LLM) comes in.","source":"@site/docs/module-4-vla/llm-cognitive-planning.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/llm-cognitive-planning","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/llm-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/Muskanateeq/Physical-AI-Humanoid-Robotics-Book/tree/main/docs/module-4-vla/llm-cognitive-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"llm-cognitive-planning","title":"Chapter 13: LLM Cognitive Planning","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 12: Voice-to-Action with Whisper","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/voice-to-action-whisper"},"next":{"title":"Final Chapter: Capstone Project - The Autonomous Humanoid","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/capstone-project-autonomous-humanoid"}}');var r=t(4848),o=t(8453),a=t(7293);const s={id:"llm-cognitive-planning",title:"Chapter 13: LLM Cognitive Planning",sidebar_position:3},l="3. LLM Cognitive Planning",c={},d=[{value:"The VLA Architecture",id:"the-vla-architecture",level:2},{value:"Step 1: Install OpenAI Library",id:"step-1-install-openai-library",level:2},{value:"Step 2: Create the Planner Node (<code>llm_planner_node.py</code>)",id:"step-2-create-the-planner-node-llm_planner_nodepy",level:2},{value:"How It Works",id:"how-it-works",level:3},{value:"Exercise: Test the Planner Node",id:"exercise-test-the-planner-node",level:3}];function h(n){const e={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"3-llm-cognitive-planning",children:"3. LLM Cognitive Planning"})}),"\n",(0,r.jsxs)(e.p,{children:["Your robot can now hear commands. But how does it ",(0,r.jsx)(e.em,{children:"understand"}),' "clean the room" and know what actions to take? This is where Cognitive Planning using a Large Language Model (LLM) comes in.']}),"\n",(0,r.jsx)(e.p,{children:"In this section, we will build the brain of our operation: a ROS 2 node that:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["Subscribes to the ",(0,r.jsx)(e.code,{children:"/voice_command"})," topic."]}),"\n",(0,r.jsx)(e.li,{children:"Constructs a carefully engineered prompt for an LLM (like OpenAI's GPT series)."}),"\n",(0,r.jsx)(e.li,{children:"Sends the command to the LLM and asks it to return a structured, step-by-step plan using only actions the robot knows how to do."}),"\n",(0,r.jsx)(e.li,{children:"Validates the plan to ensure it's safe to execute."}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"the-vla-architecture",children:"The VLA Architecture"}),"\n",(0,r.jsx)(e.p,{children:"Before we code, it's crucial to understand the entire Vision-Language-Action (VLA) data flow. The following diagram illustrates how information moves through our system, from your voice to the robot's physical actions."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-text",children:"+-----------------------+      +--------------------------+\r\n|   User Voice Command  |      |   (Humanoid Robot Env)   |\r\n+-----------------------+      +--------------------------+\r\n           |                                  ^\r\n           v                                  | (6. Action Execution)\r\n+-----------------------+      +--------------------------+\r\n| ReSpeaker Mic Array   |      | ROS 2 Action Servers     |\r\n| (Captures Audio)      |      | (Navigate, Pick, Place)  |\r\n+-----------------------+      +--------------------------+\r\n           |                                  ^\r\n(1. Audio Stream)                             |\r\n           v                                  |\r\n+-------------------------------------------------------------+\r\n| NVIDIA Jetson Orin Nano (ROS 2 Humble)                      |\r\n|                                                             |\r\n|  +-----------------------+      +-----------------------+   |\r\n|  | Whisper STT Node      |-----\x3e| Planner Node          |---'\r\n|  | (Transcribes Audio)   |      | (Cognitive Engine)    | (5. Calls Actions)\r\n|  +-----------------------+      +-----------------------+   |\r\n|           | (2. Text Command)            | (4. Validates Plan) |\r\n|           |                              |                     |\r\n|           '------------------------------' (3. Sends Prompt)  |\r\n|                                            |                   |\r\n|                                            v                   |\r\n+-------------------------------------------------------------+\r\n                                     +-----------------------+\r\n                                     |  LLM API (e.g., OpenAI) |\r\n                                     |  (Returns JSON Plan)    |\r\n                                     +-----------------------+\n"})}),"\n",(0,r.jsx)(e.h2,{id:"step-1-install-openai-library",children:"Step 1: Install OpenAI Library"}),"\n",(0,r.jsx)(e.p,{children:"We'll use OpenAI's API to access their powerful language models."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"pip install openai\n"})}),"\n",(0,r.jsxs)(e.h2,{id:"step-2-create-the-planner-node-llm_planner_nodepy",children:["Step 2: Create the Planner Node (",(0,r.jsx)(e.code,{children:"llm_planner_node.py"}),")"]}),"\n",(0,r.jsx)(e.p,{children:'This node is the "Cognitive Engine" from our diagram. It\'s responsible for thinking, planning, and validating.'}),"\n",(0,r.jsxs)(a.A,{type:"danger",title:"CRITICAL: API Key Security",children:[(0,r.jsxs)(e.p,{children:["This time, an API key is ",(0,r.jsx)(e.strong,{children:"REQUIRED"}),". Before running this node, you MUST set your OpenAI API key as an environment variable. ",(0,r.jsx)(e.strong,{children:"DO NOT a hardcode the key in the script."})]}),(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# In your terminal, or added to your ~/.bashrc file\r\nexport OPENAI_API_KEY='sk-...'\n"})})]}),"\n",(0,r.jsx)(e.p,{children:"Here is the complete code for the planner node."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# llm_planner_node.py\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom openai import OpenAI\r\nimport os\r\nimport json\r\n\r\nclass LLMPlannerNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'llm_planner_node\')\r\n        self.get_logger().info(\'LLM Planner Node started.\')\r\n\r\n        # --- OpenAI Client Initialization ---\r\n        try:\r\n            self.client = OpenAI(api_key=os.environ[\'OPENAI_API_KEY\'])\r\n        except KeyError:\r\n            self.get_logger().fatal("OPENAI_API_KEY environment variable not set! Shutting down.")\r\n            raise Exception("OPENAI_API_KEY not set")\r\n\r\n        # --- Robot\'s Known Actions (The "Action Manifest") ---\r\n        # This is a critical part of the prompt. It tells the LLM what the robot can do.\r\n        self.action_manifest = """\r\n        [\r\n            {"name": "navigate_to", "description": "Moves the robot to a specific, predefined location.", "parameters": [{"name": "location", "type": "string", "enum": ["kitchen", "living_room", "charging_dock"]}]},\r\n            {"name": "find_object", "description": "Looks for an object of a certain type and returns its ID if found.", "parameters": [{"name": "object_type", "type": "string", "enum": ["bottle", "cup", "book"]}]},\r\n            {"name": "pickup_object", "description": "Picks up an object using its ID.", "parameters": [{"name": "object_id", "type": "string"}]},\r\n            {"name": "place_object", "description": "Places the held object at a location.", "parameters": [{"name": "location", "type": "string", "enum": ["table", "trash_bin"]}]}\r\n        ]\r\n        """\r\n        self.known_actions = {action[\'name\'] for action in json.loads(self.action_manifest)}\r\n\r\n        # --- ROS 2 Subscriber ---\r\n        self.subscription = self.create_subscription(\r\n            String,\r\n            \'/voice_command\',\r\n            self.command_callback,\r\n            10)\r\n        self.get_logger().info("Subscribed to /voice_command. Awaiting commands.")\r\n\r\n    def command_callback(self, msg):\r\n        command = msg.data\r\n        self.get_logger().info(f"Received command: \'{command}\'")\r\n        self.get_logger().info("Requesting a plan from LLM...")\r\n        \r\n        # Construct the prompt for the LLM\r\n        system_prompt = f"""\r\nYou are a helpful robot assistant. Your task is to break down a user\'s command into a series of robotic actions.\r\nYou can ONLY use the functions available in the action manifest provided.\r\nRespond with a valid JSON array of action objects. Each object should have a \'function\' name and a \'parameters\' object.\r\nDo not add any explanations or conversational text in your response. Only the JSON array.\r\nIf the command cannot be fulfilled with the available actions, respond with an empty JSON array [].\r\n\r\nAction Manifest:\r\n{self.action_manifest}\r\n"""\r\n\r\n        try:\r\n            # --- Send request to LLM ---\r\n            response = self.client.chat.completions.create(\r\n                model="gpt-4-turbo",\r\n                messages=[\r\n                    {"role": "system", "content": system_prompt},\r\n                    {"role": "user", "content": command}\r\n                ],\r\n                response_format={"type": "json_object"}\r\n            )\r\n            \r\n            plan_json_str = response.choices[0].message.content\r\n            self.get_logger().info(f"LLM proposed plan (raw): {plan_json_str}")\r\n            \r\n            # --- Parse and Validate the Plan ---\r\n            plan = json.loads(plan_json_str).get(\'plan\', []) # Expecting {"plan": [...]}\r\n            if self.is_plan_valid(plan):\r\n                self.get_logger().info(f"Plan is valid! Executing plan...")\r\n                self.execute_plan(plan)\r\n            else:\r\n                self.get_logger().error("LLM returned an invalid or unsafe plan. Aborting.")\r\n\r\n        except Exception as e:\r\n            self.get_logger().error(f"An error occurred while communicating with OpenAI or processing the plan: {e}")\r\n\r\n    def is_plan_valid(self, plan):\r\n        """\r\n        Safety First! This function validates the plan from the LLM.\r\n        """\r\n        if not isinstance(plan, list):\r\n            self.get_logger().error("Validation failed: Plan is not a list.")\r\n            return False\r\n        \r\n        for action in plan:\r\n            if \'function\' not in action or \'parameters\' not in action:\r\n                self.get_logger().error(f"Validation failed: Action missing \'function\' or \'parameters\': {action}")\r\n                return False\r\n            if action[\'function\'] not in self.known_actions:\r\n                self.get_logger().error(f"Validation failed: Unknown function \'{action[\'function\']}\'")\r\n                return False\r\n        \r\n        self.get_logger().info("Plan validation successful.")\r\n        return True\r\n\r\n    def execute_plan(self, plan):\r\n        """\r\n        This is a placeholder for where you would call ROS 2 action servers.\r\n        In the final capstone, this will trigger real robot actions.\r\n        """\r\n        self.get_logger().info("--- STARTING PLAN EXECUTION ---")\r\n        for i, action in enumerate(plan):\r\n            self.get_logger().info(f"Step {i+1}: Executing {action[\'function\']} with params {action[\'parameters\']}")\r\n            # In a real system, you would do something like:\r\n            # future = self.action_client.send_goal_async(action_goal)\r\n            # rclpy.spin_until_future_complete(self, future)\r\n            # self.get_logger().info("Step completed.")\r\n        self.get_logger().info("--- PLAN EXECUTION FINISHED ---")\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    try:\r\n        llm_planner_node = LLMPlannerNode()\r\n        rclpy.spin(llm_planner_node)\r\n    except Exception as e:\r\n        print(f"Node failed to initialize or run: {e}")\r\n    finally:\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,r.jsx)(e.h3,{id:"how-it-works",children:"How It Works"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Action Manifest"}),": This is the most important part of the prompt. It's a JSON string that explicitly tells the LLM what functions the robot can perform and what parameters they accept. This constrains the LLM, preventing it from hallucinating impossible actions."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"System Prompt"}),': We give the LLM a persona ("You are a helpful robot assistant") and clear instructions: use only the functions provided and respond ',(0,r.jsx)(e.em,{children:"only"})," with a JSON array. This is called prompt engineering."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"LLM Call"}),": When a command is received, the node sends the system prompt and the user command to the OpenAI API, requesting a JSON response."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Validation"}),": This is a critical safety step. The ",(0,r.jsx)(e.code,{children:"is_plan_valid"})," function checks the LLM's response to ensure it's a list and that every action in the list is one we defined in our manifest. ",(0,r.jsx)(e.strong,{children:"Never trust an LLM's output without validation."})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Execution (Placeholder)"}),": The ",(0,r.jsx)(e.code,{children:"execute_plan"})," function currently just prints the steps. In the final capstone project, this is where you will make calls to your ROS 2 action clients (for navigation, manipulation, etc.) to make the robot move."]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"exercise-test-the-planner-node",children:"Exercise: Test the Planner Node"}),"\n",(0,r.jsx)(e.p,{children:"For this exercise, we will use the two terminals from the previous step."}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["Save the code above as ",(0,r.jsx)(e.code,{children:"llm_planner_node.py"})," in your package."]}),"\n",(0,r.jsxs)(e.li,{children:["Make sure your ",(0,r.jsx)(e.code,{children:"OPENAI_API_KEY"})," is exported."]}),"\n",(0,r.jsx)(e.li,{children:"Build and source your workspace again."}),"\n",(0,r.jsxs)(e.li,{children:["In terminal 1, run the voice transcriber node:","\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"ros2 run your_package_name voice_transcriber_node\n"})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["In terminal 2, run the new planner node:","\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"ros2 run your_package_name llm_planner_node\n"})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["Now, speak a command like: ",(0,r.jsx)(e.strong,{children:'"Find the bottle and then go to the kitchen."'}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Watch the first terminal. You should see it transcribe your text."}),"\n",(0,r.jsx)(e.li,{children:"Watch the second terminal. It should log that it received the command, requested a plan, and then print the step-by-step plan it received from the LLM."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"You have now built a system that can listen and think. The final step is to connect this thinking engine to the robot's body."})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(h,{...n})}):h(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>s});var i=t(6540);const r={},o=i.createContext(r);function a(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:a(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);